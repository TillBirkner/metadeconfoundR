% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MetaDeconfound.R
\name{MetaDeconfound}
\alias{MetaDeconfound}
\title{Metadeconfound}
\usage{
MetaDeconfound(
  featureMat,
  metaMat,
  nnodes = 1,
  adjustMethod = "fdr",
  robustCutoff = 5,
  QCutoff = 0.1,
  DCutoff = 0,
  PHS_cutoff = 0.05,
  NA_imputation = "remove",
  logfile = NULL,
  intermediateOutput = NULL,
  startStop = NA,
  QValues = NA,
  DValues = NA,
  minQValues = NULL,
  deconfT = NULL,
  deconfF = NULL,
  doConfs = 0,
  doRanks = NA,
  randomVar = NA,
  robustCutoffRho = NULL,
  typeCategorical = NULL,
  typeContinuous = NULL,
  logistic = FALSE,
  rawCounts = FALSE,
  returnLong = FALSE,
  ...
)
}
\arguments{
\item{featureMat}{a data frame with row(sample ID)
and column(feature such as metabolite or microbial OTU )
names, listing features for all samples}

\item{metaMat}{a data frame with row(sample ID) and
column(meta data such as age,BMI and all possible confounders)
names listing metadata for all samples. first column should be case status
with case=1 and control=0. All binary variables need to be in 0/1 syntax!}

\item{nnodes}{number of nodes/cores to be used for parallel processing}

\item{adjustMethod}{multiple testing p-value correction using one of the
methods of \link[stats]{p.adjust.methods}}

\item{robustCutoff}{minimal number of sample size for each covariate
in order to have sufficient power for association testing}

\item{QCutoff}{significance cutoff for q-value, DEFAULT = 0.1}

\item{DCutoff}{effect size cutoff
(either cliff's delta or spearman correlation test estimate), DEFAULT = 0}

\item{PHS_cutoff}{PostHoc Significance cutoff}

\item{NA_imputation}{Missing data treatment. "remove": remove NA containing
data from analysis (default).
"others": Impute NAs  using methods from packages MICE or AMELIA
(not yet implemented)}

\item{logfile}{name of optional logging file}

\item{intermediateOutput}{name base for optional intermediate files
(Ps, Qs and Ds from final output)}

\item{startStop}{vector of optional strings controlling which
parts of the pipeline should be executed.
("naiveStop": only naive associations will be computed, no confounder analysis is done)}

\item{QValues}{optional data.frame containing pre-computed multiple-testing corrected p-values for naive associations}

\item{DValues}{optional data.frame containing pre-computed effect sizes for naive associations}

\item{minQValues}{pessimistic qvalues, can be generated by
\link[metadeconfoundR]{ImportLongPrior}.
This dataframe of QValues is used to incorporate prior knowledge of
potential associations between individual features and metadata by supplying
QValues < QCutoff for these associations. All significant associations thus
reported will be treated as potentially confounding influences.}

\item{deconfT}{vector of metavariable names *always* to be included as potential confounder}

\item{deconfF}{vector of metavariable names *never* to be included as potential confounder}

\item{doConfs}{optional parameter for additional computation of confidence
interval of linear models in the deconfounding step
(0 = no , 1 = logging, 2 = strict)}

\item{doRanks}{optional vector of metavariable names, that should be rank
transformed when building linear models in the doconfounding step}

\item{randomVar}{optional vector of metavariable names to be treated as
random effect variables. These variabels will not be tested for naive
associations and will not be included as potential confounders,
but will be added as random effects "+ (1|variable)" into ay models being built.}

\item{robustCutoffRho}{optional robustness cutoff for continuous variables}

\item{typeCategorical}{optional character vector of metavariable names to
always be treated as categorical}

\item{typeContinuous}{optional character vector of metavariable names to
always be treated as continuous}

\item{logistic}{optional logical parameter; DEFAULT = FALSE;
Set TRUE to treat supplied features as binary instead of continuous}

\item{rawCounts}{optional logical parameter; DEFAULT = FALSE;
Set TRUE to treat supplied features as not normalized/rarefied counts;
metadeconfoundR will compute total read count per sample and include this
information in the modelling steps. WARNING: naive associations computed in
first part of metadeconfoundR are reliant on normalized/rarefied data.
Please split your analysis up into 2 parts as shown in the documentation
when using this mode..}

\item{returnLong}{DEFAULT = FALSE; Set TRUE to get output in one long
format data.frame instead of list of four wide format data.frames}

\item{...}{for additional arguments used internally (development/debugging)}
}
\value{
list with elements (or data.frame with columns, when returnLong = TRUE) Ds = effectsize,
Ps = uncorrected p-value for naive association,
Qs = multiple testing corrected p-value/fdr,
and status = confounding status for all
feature <=> covariate combinations with following categories:
(NS = not significant, OK_sd = strictly deconfounded, OK_nc = no covariates,
OK_d = doubtful, AD = ambiguously deconfounded, C: followed by comma
separated covariate names = confounded by listed covariates)\cr

Can be plotted using \link[metadeconfoundR]{BuildHeatmap}.
}
\description{
Metadeconfound checks all feature <-> covariate combinations for
counfounding effects of covariates on feature <-> effect correlation
}
\details{
for more details and explanations please see the vignette.
}
\examples{
data(reduced_feature)
data(metaMatMetformin)
\donttest{
example_output <- MetaDeconfound(featureMat = reduced_feature,
                                  metaMat = metaMatMetformin)}

}
